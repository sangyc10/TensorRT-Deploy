# ä¸€ã€TensorTRéƒ¨ç½²YoloV5 â€“äº¤é€šæ£€æµ‹é¡¹ç›®

## 1ã€ä¸‹è½½YOLOV5

YOLOv5æ˜¯ä¸€ç§æµè¡Œçš„ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼Œå®ƒæä¾›äº†ä¸åŒå¤§å°çš„å˜ä½“ä»¥é€‚åº”ä¸åŒçš„åº”ç”¨éœ€æ±‚å’Œç¡¬ä»¶é™åˆ¶ã€‚è¿™äº›å˜ä½“ä¸»è¦åœ¨æ¨¡å‹çš„å®½åº¦å’Œæ·±åº¦ä¸Šæœ‰æ‰€ä¸åŒï¼Œå½±å“äº†æ¨¡å‹çš„å¤æ‚åº¦ã€é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

å®‰è£…ï¼š

```shell
# å…‹éš†åœ°å€
git clone https://github.com/ultralytics/yolov5.git
# è¿›å…¥ç›®å½•
cd yolov5	
# é€‰æ‹©åˆ†æ”¯ï¼ˆæ­¤åˆ†æ”¯ä¸ºæˆªè‡³æ­¤æ–‡æ¡£æ’°å†™YOLOV5çš„æœ€æ–°ç‰ˆæœ¬ï¼‰
git checkout db125a20175384d75560dc9af7fb1100d67213fe
# å®‰è£…ä¾èµ–ç¯å¢ƒ
pip3 install -r requirements.txt
# æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå®‰è£…ultralytics
pip3 install ultralytics
```

ä¸‹è½½åŸYOLOV5é¢„è®­ç»ƒæƒé‡ï¼š

[ä¸‹è½½é¢„è®­ç»ƒæƒé‡](https://github.com/ultralytics/yolov5)ï¼Œå°†é¢„è®­ç»ƒæƒé‡ä¿å­˜åœ¨weightsæ–‡ä»¶å¤¹ä¸‹ã€‚

æµ‹è¯•YOLOV5æ˜¯å¦å®‰è£…æˆåŠŸï¼š

```shell
python3 detect.py --source ./data/images/ --weights weights/yolov5s.pt --conf-thres 0.4
```



##  2ã€è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†

### 2.1 æ•°æ®é›†ä»‹ç»

Kaggleä¸‹è½½æ•°æ®é›†ï¼š[Dhaka-AI-Yolo-Formatted-Dataset](https://www.kaggle.com/datasets/aifahim/dhakaaiyoloformatteddataset)

è¯¥æ•°æ®é›†ä¸ºäº¤é€šåœºæ™¯ä¸‹ï¼Œäº¤é€šå·¥å…·ç±»åˆ«æ£€æµ‹ã€‚åŒ…å«è®­ç»ƒé›†2390å¼ å›¾ç‰‡ï¼ŒéªŒè¯é›†600å¼ å›¾ç‰‡ï¼Œå›¾åƒåˆ†è¾¨ç‡ä¸º1024*1024ã€‚å…±21ä¸ªç±»åˆ«ã€‚

#### 2.1.1 åˆ›å»ºyamlæ–‡ä»¶

åˆ›å»ºtraffic_vehicle.yamlæ–‡ä»¶ï¼š

```bas
path: ../datasets/traffic_vehicle 	# dataset root dir
train: images/train 				# train images (relative to 'path')
val: images/train					# val images (relative to 'path')
test: # test images (optional)

# Classes
names: 
  0: ambulance
  1: auto rickshaw
  2: bicycle
  3: bus
  4: car
  5: garbagevan
  6: human hauler
  7: minibus
  8: minivan
  9: motorbike
  10: Pickup
  11: army vehicle
  12: policecar
  13: rickshaw
  14: scooter
  15: suv
  16: taxi
  17: three wheelers (CNG)
  18: truck
  19: van
  20: wheelbarrow
```

å°†traffic_vehicle.yamlæ–‡ä»¶å¤åˆ¶åˆ°yolov5/dataç›®å½•ä¸‹ã€‚

#### 2.1.2 ç»„ç»‡ç›®å½•ç»“æ„

æ•°æ®é›†ç›®å½•ç»“æ„éµå¾ªYOLOæ•°æ®é›†è§„åˆ™ã€‚

```bash
.
â”œâ”€â”€ datasets
â”‚   â””â”€â”€ traffic_vehicle
â”‚       â”œâ”€â”€ images
â”‚       â”‚   â”œâ”€â”€ train
â”‚       â”‚   â””â”€â”€ val
â”‚       â””â”€â”€ labels
â”‚           â”œâ”€â”€ train
â”‚           â”œâ”€â”€ train.cache
â”‚           â””â”€â”€ val
â””â”€â”€ yolov5
```

æ³¨æ„ï¼š**datasets**ä¸**yolov5**åœ¨åŒçº§ç›®å½•ã€‚



### 2.2 è®­ç»ƒè‡ªå®šä¹‰æ•°æ®é›†

#### 2.1.1 é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹

YOLOv5æä¾›å¤šç§æ¨¡å‹ï¼š

1. **YOLOv5s (small)**
2. **YOLOv5m (medium)**
3. **YOLOv5l (large)**
4. **YOLOv5x (extra large)**

æœ¬é¡¹ç›®é€‰æ‹©YOLOv5sä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼ï¼

å¤åˆ¶modelsæ–‡ä»¶å¤¹ä¸‹å¯¹åº”æ¨¡å‹**yaml**æ–‡ä»¶ï¼Œé‡æ–°å‘½åï¼Œä¾‹å¦‚ï¼Œæœ¬é¡¹ç›®é€‰æ‹©**YOLOv5s**ä½œä¸ºé¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ™å‘½åä¸º**yolov5s_traffic_vehicle.yaml**ã€‚

éœ€è¦ä¿®æ”¹ç±»åˆ«æ•°é‡ä¸ºæœ¬é¡¹ç›®çš„21ä¸ªç±»åˆ«ï¼Œyolov5s_traffic_vehicle.yamlå†…å®¹å¦‚ä¸‹ï¼š

```bash
# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license

# Parameters
nc: 21 # number of classes
depth_multiple: 0.33 # model depth multiple
width_multiple: 0.50 # layer channel multiple
anchors:
  - [10, 13, 16, 30, 33, 23] # P3/8
  - [30, 61, 62, 45, 59, 119] # P4/16
  - [116, 90, 156, 198, 373, 326] # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [
    [-1, 1, Conv, [64, 6, 2, 2]], # 0-P1/2
    [-1, 1, Conv, [128, 3, 2]], # 1-P2/4
    [-1, 3, C3, [128]],
    [-1, 1, Conv, [256, 3, 2]], # 3-P3/8
    [-1, 6, C3, [256]],
    [-1, 1, Conv, [512, 3, 2]], # 5-P4/16
    [-1, 9, C3, [512]],
    [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32
    [-1, 3, C3, [1024]],
    [-1, 1, SPPF, [1024, 5]], # 9
  ]

# YOLOv5 v6.0 head
head: [
    [-1, 1, Conv, [512, 1, 1]],
    [-1, 1, nn.Upsample, [None, 2, "nearest"]],
    [[-1, 6], 1, Concat, [1]], # cat backbone P4
    [-1, 3, C3, [512, False]], # 13

    [-1, 1, Conv, [256, 1, 1]],
    [-1, 1, nn.Upsample, [None, 2, "nearest"]],
    [[-1, 4], 1, Concat, [1]], # cat backbone P3
    [-1, 3, C3, [256, False]], # 17 (P3/8-small)

    [-1, 1, Conv, [256, 3, 2]],
    [[-1, 14], 1, Concat, [1]], # cat head P4
    [-1, 3, C3, [512, False]], # 20 (P4/16-medium)

    [-1, 1, Conv, [512, 3, 2]],
    [[-1, 10], 1, Concat, [1]], # cat head P5
    [-1, 3, C3, [1024, False]], # 23 (P5/32-large)

    [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5)
  ]

```

#### 2.1.3 è®­ç»ƒ

ä¸‹è½½å¯¹åº”çš„é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œæ”¾åˆ°`weights`ç›®å½•ä¸‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹è®­ç»ƒï¼š

```shell
python3 ./train.py --data ./data/traffic_vehicle.yaml --cfg ./models/yolov5s_traffic_vehicle.yaml --weights ./weights/yolov5s.pt --batch-size 32 --epochs 300 --name epochs300_1 --project yolo_traffic_vehicle
```

#### 2.1.4 è¯„ä¼°è®­ç»ƒç»“æœ

é€‰æ‹©ä¸€å¼ å›¾ç‰‡æ£€æµ‹ï¼ŒæŸ¥çœ‹æ•ˆæœ

```she
python3 detect.py --source ../datasets/traffic_vehicle/images/val/Navid_254.jpg --weights ./yolo_traffic_vehicle/epochs300_1/weights/best.pt --conf-thres 0.4
```

<img src="images/Navid_254.jpg" style="zoom: 67%;" />





```bash
python3 val.py --data  ./data/traffic_vehicle.yaml  --weights ./yolo_traffic_vehicle/epochs300_1/weights/best.pt --batch-size 32
```

```bash
YOLOv5s_traffic_vehicle summary: 157 layers, 7066762 parameters, 0 gradients, 15.9 GFLOPs
val: Scanning /home/***/Course/tensorrtYoloV5/datasets/traffic_vehicle/labels/val.cache... 600 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:06<00:00,  3.12it/s]
                   all        600       5090       0.72      0.437      0.488      0.326
             ambulance        600         22      0.697      0.318      0.356      0.235
         auto rickshaw        600         70      0.858      0.614      0.741      0.522
               bicycle        600         97      0.733      0.268      0.359      0.159
                   bus        600        698      0.867      0.665      0.763      0.525
                   car        600       1097      0.791      0.713      0.751      0.519
            garbagevan        600          1          1          0          0          0
          human hauler        600         40      0.872       0.34      0.507      0.339
               minibus        600         18      0.711      0.333      0.344      0.203
               minivan        600        158      0.519       0.43      0.412      0.285
             motorbike        600        466       0.79      0.605      0.661      0.353
                Pickup        600        238      0.589      0.433      0.482       0.33
          army vehicle        600          6      0.855        0.5      0.554       0.46
             policecar        600          6          0          0     0.0865     0.0618
              rickshaw        600        851      0.814      0.578      0.658      0.397
               scooter        600          7      0.626      0.286      0.191     0.0749
                   suv        600        182      0.609      0.385      0.439      0.324
                  taxi        600          7       0.97      0.571      0.674      0.516
  three wheelers (CNG)        600        680      0.898      0.749      0.808      0.548
                 truck        600        275      0.792      0.607      0.681      0.464
                   van        600        148      0.594      0.514      0.492      0.335
           wheelbarrow        600         23      0.528      0.261      0.293      0.203
Speed: 0.4ms pre-process, 4.6ms inference, 0.4ms NMS per image at shape (32, 3, 640, 640)


```





## 3ã€å¯¼å‡ºONNX

åœ¨æœ¬é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**TensoRT plugin**æ¥ä»£æ›¿åŸæ¥**YOLOV5**ä»£ç ä¸­çš„**decode**æ“ä½œï¼Œå¦‚æœä¸æ›¿æ¢ï¼Œè¿™éƒ¨åˆ†è¿ç®—å°†å½±å“æ•´ä½“æ€§èƒ½ã€‚ä¸ºäº†è®©`tensorrt`èƒ½å¤Ÿè¯†åˆ«å¹¶åŠ è½½æˆ‘ä»¬é¢å¤–æ·»åŠ çš„**plugin**ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹**YOLOV5**ä»£ç ä¸­å¯¼å‡º**onnx**æ¨¡å‹çš„éƒ¨åˆ†ã€‚

### 3.1 å¯¼å‡ºæœªä¿®æ”¹çš„YOLOV5s ONNXæ¨¡å‹

```shel
python3 export.py --weights yolo_traffic_vehicle/epochs120_1/weights/best.pt --include onnx --simplify
```

å¯ä»¥é€šè¿‡Netronè¿›è¡ŒæŸ¥çœ‹å¯¼å‡ºç»“æœã€‚

### 3.2 å¯¼å‡ºéœ€è¦çš„YOLOV5s ONNXæ¨¡å‹

#### 3.2.1 ä¿®æ”¹decodeå¤„ä»£ç 

åœ¨`models/yolo.py`æ–‡ä»¶ä¸­95è¡Œï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹**class Detect**çš„forwardæ–¹æ³•ï¼Œåˆ é™¤å…¶**decode**è¿ç®—ï¼Œç›´æ¥è¾“å‡ºç½‘ç»œç»“æœã€‚åœ¨åé¢çš„**TensorRT**éƒ¨ç½²ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨**decode plugin**æ¥è¿›è¡Œ**decode**æ“ä½œï¼Œå¹¶ç”¨**GPU**åŠ é€Ÿã€‚ä¿®æ”¹å†…å®¹å¦‚ä¸‹ï¼š


ä¿®æ”¹åçš„**forward**å‡½æ•°ä¸ºï¼š

```python
def forward(self, x):
    z = []  # inference output
    for i in range(self.nl):
        x[i] = self.m[i](x[i])  # conv
        y = x[i].sigmoid()
        z.append(y)
	return z
```

ä¿®æ”¹åçš„**_make_grid**å‡½æ•°ä¸ºï¼š

```python
def _make_grid(self, nx=20, ny=20, i=0, torch_1_10=check_version(torch.__version__, "1.10.0")):
        """Generates a mesh grid for anchor boxes with optional compatibility for torch versions < 1.10."""
	d = self.anchors[i].device
    t = torch.int32
    shape = 1, self.na, ny, nx, 2  # grid shape
    y, x = torch.arange(ny, device=d, dtype=t), torch.arange(nx, device=d, dtype=t)
    yv, xv = torch.meshgrid(y, x, indexing="ij") if torch_1_10 else torch.meshgrid(y, x)  # torch>=0.7 compatibility
    grid = torch.stack((xv, yv), 2).expand(shape) - 0.5  # add grid offset, i.e. y = 2.0 * x - 0.5
    anchor_grid = (self.anchors[i] * self.stride[i]).view((1, self.na, 1, 1, 2)).expand(shape)
    return grid, anchor_grid
```

#### 3.2.2 ä¿®æ”¹export.pyå¯¼å‡ºå‡½æ•°ä»£ç 

æ³¨é‡Šä»¥ä¸‹ä¸¤è¡Œä»£ç ï¼ˆæ›´æ”¹æºæ–‡ä»¶ä½ç½®å¯èƒ½å˜åŒ–ï¼Œæ‰€ä»¥ä¸æŒ‡å‡ºå…·ä½“è¡Œæ•°ï¼‰ï¼š

```python
# shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # model output shape

# LOGGER.info(f"\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)")
```

ä¿®æ”¹export_onnxå‡½æ•°ï¼š

```python
def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr("ONNX:")):
    """Exports a YOLOv5 model to ONNX format with dynamic axes and optional simplification."""
    check_requirements("onnx>=1.12.0")
    import onnx

    LOGGER.info(f"\n{prefix} starting export with onnx {onnx.__version__}...")
    f = str(file.with_suffix(".onnx"))

    output_names = ["output0", "output1"] if isinstance(model, SegmentationModel) else ["p3", "p4", "p5"]
    if dynamic:
        dynamic = {"images": {0: "batch", 2: "height", 3: "width"}}  # shape(1,3,640,640)
        if isinstance(model, SegmentationModel):
            dynamic["output0"] = {0: "batch", 1: "anchors"}  # shape(1,25200,85)
            dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # shape(1,32,160,160)
        elif isinstance(model, DetectionModel):
            dynamic["p3"] = {0: "batch", 2: "height", 3: "width"} 
            dynamic["p4"] = {0: "batch", 2: "height", 3: "width"} 
            dynamic["p5"] = {0: "batch", 2: "height", 3: "width"} 

    torch.onnx.export(
        model.cpu() if dynamic else model,  # --dynamic only compatible with cpu
        im.cpu() if dynamic else im,
        f,
        verbose=False,
        opset_version=opset,
        do_constant_folding=True,  # WARNING: DNN inference with torch>=1.12 may require do_constant_folding=False
        input_names=["images"],
        output_names=output_names,
        dynamic_axes=dynamic or None,
    )

    # Checks
    model_onnx = onnx.load(f)  # load onnx model
    onnx.checker.check_model(model_onnx)  # check onnx model
    # onnx.save(model_onnx, f)

    # Simplify
    if simplify:
        try:
            cuda = torch.cuda.is_available()
            check_requirements(("onnxruntime-gpu" if cuda else "onnxruntime", "onnx-simplifier>=0.4.1"))
            import onnxsim

            LOGGER.info(f"{prefix} simplifying with onnx-simplifier {onnxsim.__version__}...")
            model_onnx, check = onnxsim.simplify(model_onnx)
            assert check, "assert check failed"
            onnx.save(model_onnx, f)
        except Exception as e:
            LOGGER.info(f"{prefix} simplifier failure: {e}")
    #return f, model_onnx
    
    import onnx_graphsurgeon as onnx_gs
    import numpy as np
    yolo_graph = onnx_gs.import_onnx(model_onnx)
    p3 = yolo_graph.outputs[0]
    p4 = yolo_graph.outputs[1]
    p5 = yolo_graph.outputs[2]
    decode_out_0 = onnx_gs.Variable(
        "DecodeNumDetection",
        dtype=np.int32
    )
    decode_out_1 = onnx_gs.Variable(
        "DecodeDetectionBoxes",
        dtype=np.float32
    )
    decode_out_2 = onnx_gs.Variable(
        "DecodeDetectionScores",
        dtype=np.float32
    )
    decode_out_3 = onnx_gs.Variable(
        "DecodeDetectionClasses",
        dtype=np.int32
    )

    decode_attrs = dict()

    decode_attrs["max_stride"] = int(max(model.stride))
    decode_attrs["num_classes"] = model.model[-1].nc
    decode_attrs["anchors"] = [float(v) for v in [10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326]]
    decode_attrs["prenms_score_threshold"] = 0.25

    decode_plugin = onnx_gs.Node(
        op="YoloLayer_TRT",
        name="YoloLayer",
        inputs=[p3, p4, p5],
        outputs=[decode_out_0, decode_out_1, decode_out_2, decode_out_3],
        attrs=decode_attrs
    )

    yolo_graph.nodes.append(decode_plugin)
    yolo_graph.outputs = decode_plugin.outputs
    yolo_graph.cleanup().toposort()
    model_onnx = onnx_gs.export_onnx(yolo_graph)

    # Metadata
    d = {'stride': int(max(model.stride)), 'names': model.names}
    for k, v in d.items():
        meta = model_onnx.metadata_props.add()
        meta.key, meta.value = k, str(v)

    onnx.save(model_onnx, f)
    LOGGER.info(f'{prefix} export success, saved as {f} ({file_size(f):.1f} MB)')
    return f
```




å¯¼å‡ºä¿®æ”¹åçš„ONNXï¼š

```shell
python3 export.py --data data/traffic_vehicle.yaml --weights weights/traffic_vehicle.pt --include onnx --simplify
```

ä¸»è¦ä¿®æ”¹çš„æ˜¯YOLOV5 decodeéƒ¨åˆ†ï¼š

<img src="images/decode.png" alt="decode" style="zoom:80%;" />

æ¨¡å‹æœ‰4ä¸ªè¾“å‡ºï¼š

- DecodeNumDetectionï¼šæ£€æµ‹æ¡†æ•°é‡
- DecodeDetectionBoxesï¼šæ£€æµ‹æ¡†åæ ‡
- DecodeDetectionScoresï¼šæ£€æµ‹æ¡†ç½®ä¿¡åº¦
- DecodeDetectionClassesï¼šæ£€æµ‹æ¡†ç±»åˆ«

## 4ã€TensorRTæ¨¡å‹æ„å»º

### 4.1 é…ç½®æ–‡ä»¶

åœ¨configæ–‡ä»¶å¤¹ä¸‹ï¼Œconfig.yamlé…ç½®æ–‡ä»¶ï¼š

```yaml
onnxPath: ./models/onnx/traffic_vehicle.onnx

logging:
  level: 4          # FATAL:0  ERROR:1   WARN:2  INFO:3  VERB:4  DEBUG:5

model_params:
  image:
    - 640
    - 640
    - 3
  num_cls: 1000
  task: 1            # CLASSIFICATION:0  DETECTION:1  SEGMENTATION:2  MULTITASK:3
  device: 1          # CPU:0  GPU:1
  precision: 0       # FP32:0 FP16:1  INT8:2
  calibration_list: ./calibration/calibration_list_traffic.txt
  calibration_table: ./calibration/calibration_table_traffic.txt
  calibration_batchsize: 64

images_path:
  - ./data/source/33.jpg
  - ./data/source/40.jpg
  - ./data/source/47.jpg
  - ./data/source/52.jpg
  - ./data/source/52.jpg
  - ./data/source/135.jpg
  - ./data/source/145.jpg
  - ./data/source/147.jpg
```

æ ¹æ®é…ç½®æ–‡ä»¶è¿›è¡Œå‚æ•°è®¾ç½®ï¼Œé¿å…é¢‘ç¹æ›´æ”¹æºæ–‡ä»¶ï¼Œé‡å¤ç¼–è¯‘ã€‚

### 4.2 CMakeç¼–è¯‘

æ„å»ºç³»ç»Ÿæ–‡ä»¶ç”Ÿæˆåœ¨æŒ‡å®šçš„`build`ç›®å½•ä¸­:

```bash
cmake -S . -B build
```
ç¼–è¯‘ï¼Œç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶åˆ°./binç›®å½•ä¸‹ï¼š

```bash
cmake --build build
```

### 4.3 æ‰§è¡Œå¯æ‰§è¡Œæ–‡ä»¶ç”Ÿæˆengineå¹¶æ¨ç†

```bash
./bin/trt-infer
```

æ‰§è¡Œä¸Šè¿°å‘½ä»¤ï¼Œè¿›è¡Œå›¾åƒçš„TensorRTæ¨ç†ã€‚å½“ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ï¼Œä¼šè¿›è¡Œengineæ–‡ä»¶çš„ç”Ÿæˆï¼Œéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç”Ÿæˆä¸€æ¬¡engineæ–‡ä»¶åï¼Œä¹‹åæ‰§è¡Œï¼Œç³»ç»Ÿä¼šç›´æ¥åŠ è½½engineæ–‡ä»¶ã€‚

![YOLOV5-infer](images/YOLOV5-infer.png)



### 4.4 INT8æ¨ç†

FP32å’ŒFP16å¯ç›´æ¥è¿›è¡Œæ¨ç†ï¼Œå½“ä½¿ç”¨INT8é‡åŒ–æ¨ç†æ—¶ï¼Œéœ€è¦æ‰§è¡Œæ ¡å‡†æ“ä½œï¼Œæˆ‘ä»¬è¦äº‹å…ˆå‡†å¤‡å¥½æ ¡å‡†å›¾åƒã€‚



# äºŒã€TensorRTå›¾åƒåˆ†ç±»ä»»åŠ¡

å›¾åƒåˆ†ç±»ä»»åŠ¡è¾“å…¥è¾“å‡ºéƒ½å¤§è‡´ç›¸åŒï¼Œå› æ­¤ç¤ºä¾‹å¯ä»¥é€‚ç”¨å¤šä¸ªåˆ†ç±»æ¨¡å‹ã€‚

## 1ã€ä»torchvisionå¯¼å‡ºONNXæ¨¡å‹

å¯¼å‡ºåˆ†ç±»æ¨¡å‹ONNXä»£ç åœ¨**./src/python/torchvision2onnx.py**ï¼Œè¿™é‡Œæä¾›çš„æ¨¡å‹åŒ…æ‹¬**resnet50**ã€**resnet101**ã€**resnet152**ã€**vgg11**ã€**vgg19**ã€**mobilenet_v3_small**ã€**efficientnet_b0**ã€**efficientnet_v2_s**ï¼Œå¦‚éœ€å…¶ä»–æ¨¡å‹ï¼Œå¯è‡ªè¡Œæ‰©å±•ã€‚

ä»¥**vgg11**æ¨¡å‹ä¸ºä¾‹ï¼Œæ‰§è¡Œï¼ˆæ³¨æ„è·¯å¾„ï¼‰ï¼š

```python
python3 ./src/python/torchvision2onnx.py --type vgg11 --dir ./models/onnx/
```

å¯¼å‡ºonnxæ¨¡å‹ä¿å­˜åœ¨**./model/onnx/**æ–‡ä»¶å¤¹ä¸­ã€‚

## 2ã€é…ç½®æ–‡ä»¶

åœ¨configæ–‡ä»¶å¤¹ä¸‹ï¼Œconfig.yamlé…ç½®æ–‡ä»¶ï¼š

```yaml
onnxPath: ./models/onnx/vgg11.onnx

logging:
  level: 4          # FATAL:0  ERROR:1   WARN:2  INFO:3  VERB:4  DEBUG:5

model_params:
  image:
    - 224
    - 224
    - 3
  num_cls: 1000
  task: 0            # CLASSIFICATION:0  DETECTION:1  SEGMENTATION:2  MULTITASK:3
  device: 1          # CPU:0  GPU:1
  precision: 0       # FP32:0 FP16:1  INT8:2
  calibration_list: ./calibration/calibration_list_imagenet.txt
  calibration_table: ./calibration/calibration_table_imagenet.txt
  calibration_batchsize: 64

images_path:
  - ./data/source/cat.png
  - ./data/source/fox.png
  - ./data/source/eagle.png
  - ./data/source/tiny-cat.png
  - ./data/source/wolf.png
```

æ ¹æ®é…ç½®æ–‡ä»¶è¿›è¡Œå‚æ•°è®¾ç½®ï¼Œé¿å…é¢‘ç¹æ›´æ”¹æºæ–‡ä»¶ï¼Œé‡å¤ç¼–è¯‘ã€‚



## 3ã€CMakeç¼–è¯‘

æ„å»ºç³»ç»Ÿæ–‡ä»¶ç”Ÿæˆåœ¨æŒ‡å®šçš„`build`ç›®å½•ä¸­:

```bash
cmake -S . -B build
```

ç¼–è¯‘ï¼Œç”Ÿæˆå¯æ‰§è¡Œæ–‡ä»¶åˆ°./binç›®å½•ä¸‹ï¼š

```bash
cmake --build build
```

## 4ã€ æ‰§è¡Œå¯æ‰§è¡Œæ–‡ä»¶ç”Ÿæˆengineå¹¶æ¨ç†

```bash
./bin/trt-infer
```

æ‰§è¡Œä¸Šè¿°å‘½ä»¤ï¼Œè¿›è¡Œå›¾åƒçš„TensorRTæ¨ç†ã€‚å½“ç¬¬ä¸€æ¬¡æ‰§è¡Œæ—¶ï¼Œä¼šè¿›è¡Œengineæ–‡ä»¶çš„ç”Ÿæˆï¼Œéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç”Ÿæˆä¸€æ¬¡engineæ–‡ä»¶åï¼Œä¹‹åæ‰§è¡Œï¼Œç³»ç»Ÿä¼šç›´æ¥åŠ è½½engineæ–‡ä»¶ã€‚

![åˆ†ç±»](images/classification.png)

æ¨¡å‹å¯¼å‡ºæ˜¯ä¸ç»è¿‡**softmax**æ“ä½œçš„ï¼Œå› æ­¤è®¡ç®—ç½®ä¿¡åº¦ï¼Œéœ€è¦ä½¿ç”¨c++å®ç°ä¸€ä¸ª**softmax**æ­¥éª¤ã€‚

## 5ã€ INT8æ¨ç†

FP32å’ŒFP16å¯ç›´æ¥è¿›è¡Œæ¨ç†ï¼Œå½“ä½¿ç”¨INT8é‡åŒ–æ¨ç†æ—¶ï¼Œéœ€è¦æ‰§è¡Œæ ¡å‡†æ“ä½œï¼Œæˆ‘ä»¬è¦äº‹å…ˆå‡†å¤‡å¥½æ ¡å‡†å›¾åƒã€‚

